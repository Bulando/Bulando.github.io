<h2>基于概念空间模型的医疗同义词提取</h2>

<h3>摘要</h3>

本文提出了一种新的医学同义词抽取方法。我们的目标是将术语嵌入与医疗领域知识集成到医疗保健应用程序中。我们的方法的一个优点是它是非常可伸缩的。在一个有超过1M项对的数据集上的实验表明，该方法大大优于基线方法。

<h3>1 引言</h3>

构建高质量的自然语言处理系统的许多组件都依赖于同义词抽取。例如查询扩展、文本摘要[Barzilay和Elhadad, 1999]、问答[Ferrucci, 2012]和释义检测。尽管同义词抽取的价值是无可争议的，但人工构建此类资源的成本总是很高，导致知识库(KB)覆盖率低[Henriksson等人，2014]。

在医学领域，这个知识库覆盖问题更为严重，因为语言使用的变异性非常高[Meystre等人，2008]。此外，医学领域的自然语言内容也在以极高的速度增长，使人们难以理解，并在知识库中及时更新。

构建大规模医学同义词抽取系统的主要挑战是如何构建一个能够将现有的人工抽取的医学知识与隐藏在非结构化文本中的大量知识自动结合的系统。本文构建了一个包含130M个句子(20gb纯文本)的医学语料库。我们还构建了一个半监督框架来生成该语料库中每个医学术语的向量表示。我们的框架扩展了Word2Vec模型[Mikolov等人，2013]，将现有的医学知识整合到模型训练过程中。

为了对同义词的概念进行建模，我们构建了一个概念空间，该空间包含了半监督术语嵌入特征和扩展特征，扩展特征可以捕获两个术语在词嵌入空间和表面形式上的相似性。然后我们直接对这个空间应用线性分类器进行同义词提取。由于人工提取的医学知识和隐藏在非结构化文本下的知识都被编码到概念空间中，一个廉价的分类器可以产生令人满意的提取结果，使高效处理大量术语对成为可能。

我们的系统是这样设计的，即在训练过程中使用现有的医学知识和非结构化文本中的上下文。该系统可以直接应用于输入项对，而不需要考虑上下文。本文在医学同义词抽取方面的贡献主要体现在两个方面。

- 从应用的角度来看，我们确定了一些统一医学语言系统(UMLS) [Lindberg等人，1993]关系，这些关系可以映射到同义词关系(表1)，并提出了一种为该应用收集大量训练和测试数据的自动方法。我们还将我们的模型应用于一组11B医学术语对，从而产生了一个新的医学同义词知识库，其中包含了以前医疗资源中未见过的3M多同义词候选词。

- 从方法的角度，我们提出了一种半监督的术语嵌入方法，该方法可以利用现有的医学领域知识和大型语料库中的文本数据训练向量空间模型。我们还扩展了术语嵌入特征，形成了一个概念空间，并利用它来方便同义词抽取。

![医疗同义词](imagesP\医疗同义词.jpg)

表1：与同义词关系相对应，其中RO代表有关系，而不是同义的，更窄或更广的，RQ代表相关的和可能同义的，SY代表断言的同义词。

实验结果表明，我们的同义词抽取模型具有较快的抽取速度，大大优于目前医学同义词抽取的最新方法。生成的同义词知识库还可以作为信息提取任务中现有知识库的补充。

<h3>2 相关工作</h3>

广泛的技术被应用到同义词检测,包括使用lexico syntactic模式(赫斯特,1992),聚类(布朗et al ., 1992),基于模型(他们et al ., 2004) (Nakayama et al ., 2007) (Wang和赫斯特,2009)(伦,2008)和分布语义[杜和蓝道,1997][Henriksson et al .,2014][Henriksson et al.， 2013a][Henriksson et al.， 2013b][Zeng et al.， arXiv:1506.00528v1 [cs。CL] 2015年6月1日2012]。也有人努力使用多源或集成方法来提高检测性能[Curran, 2002][Wu and Zhou, 2003][Peirsman and Geeraerts, 2009]。

向量空间模型与同义词抽取直接相关。一些方法利用低秩近似思想对获取语料库统计信息的大矩阵进行分解。这类方法中最具代表性的是潜在语义分析(Latent Semantic Analysis, LSA) [Deerwester et al.， 1990]。一些新的模型也遵循这种方法，如Hellinger PCA [Lebret and Collobert, 2014]和GloVe [Pennington et al.， 2014]。

基于神经网络的表示学习近年来受到了广泛的关注。最早的工作之一是在[Rumelhart等人，1986]完成的。这个想法被应用到语言建模(Bengio et al ., 2003),在机器学习动机的研究项目来构造向量表示为自然语言处理任务(Collobert和韦斯顿,2008)[特尼和Pantel, 2010] [Glorot et al ., 2011] [Socher et al ., 2011] [Mnih和格兰,2012][Passos et al .,2014]。

Word2Vec [Mikolov et al.， 2013]遵循同样的神经网络语言建模思想，显著简化了以往的模型，成为学习单词嵌入最有效的方法之一。在Word2Vec中，有两种方法从上下文生成输入所需的输出对:SkipGram(根据当前单词预测周围的单词)和CBOW(根据周围的单词预测当前的单词)，以及两种方法来简化训练:负采样(词汇用一个热表示表示，算法在每个训练步骤只考虑一些随机采样的负样本)和分层SoftMax(词汇用霍夫曼二叉树表示)。所以我们可以在4种不同的设置下训练Word2Vec模型，比如skip-gram+负采样。Word2Vec是我们半监督词嵌入模型的基础，我们将在第4.1节详细讨论它。

<h3>3 医学语料库，概念和同义词</h3>

<h4>3.1医学语料</h4>

我们的医学语料库包含了一组维基百科文章和MEDLINE摘要(2013版)1。我们还补充了大约20种医学期刊和书籍，如默克诊断和治疗手册。总的来说，语料库包含约130M个句子(约20G纯文本)，以及约15M个不同的词汇。

<h4>3.2医学概念</h4>

大量的医学知识已经存储在统一医学语言系统(UMLS)中[Lindberg et al.， 1993]，其中包括医学概念、定义、关系等。UMLS的2012版包含了来自160多个源词汇表的270多万个概念。每个概念都与一个称为CUI(概念唯一标识符)的唯一关键字相关联，每个CUI都与一个称为首选名称的术语相关联。UMLS由一组133个主题类别或语义类型组成，它们提供了所有CUI的一致类别。语义类型可进一步分为15个类别。这些语义组为99.5%的概念提供了UMLS元同义词典的一个分区。

需要特定于领域的解析器来准确地处理医学文本。该领域最知名的解析器包括MetaMap (Aronson, 2001)和MedicalESG(英语槽语法解析器[McCord et al.， 2012]对医学领域的一个改编)。这些工具可以检测给定句子中提到的医疗实体，并自动将每个术语与多个CUI关联起来。并不是所有的CUI都在医学文本中被积极使用。例如，在我们的语料库中，MedicalESG仅识别出150K个CUIs，而在UMLS中总共有270万个CUIs。

<h4>3.3医学同义词</h4>

同义词是两个意义非常相似的名词之间的语义关系。然而，这两个词有完全相同的意思是极其罕见的。在本文中，我们的重点是识别近义词，即两个术语在某些语境中是可以互换的[Saeed, 2009]。

UMLS 2012版本在15个类别下包含600多个关系和50M多个关系实例。每个类别包含若干关系，且每个关系具有一定数量的已知承载该关系的CUI对。从UMLS关系中，我们手动选择与同义词直接相关的子集，并在表1中总结它们。在表2中，我们列出了由这些关系提供的几个同义词示例。

![1631170753(1)](imagesP\1631170753(1).jpg)

​                                                                表2：在UMLS中几个同义词示例

<h3>4 基于概念空间模型的医学同义词抽取</h3>

在本节中，我们首先将这些项投影到一个新的向量空间，得到每个项的向量表示(第4.1节)。这是通过将来自UMLS的语义类型和语义组知识作为额外标签添加到Word2Vec模型训练过程中来实现的。在第二步(第4.2节)中，我们用额外的特征扩展结果项向量，以建模两个项之间的关系。所有这些特征共同构成一个概念空间，并与线性分类器一起用于医学同义词提取。

<h4>4.1 半监督项嵌入高水平的想法</h4>

Word2Vec模型的一个关键点是为每个词或词序列从上下文中生成一个所需的词，然后学习一个浅层神经网络来预测这个词或词序列所需的词映射。所需的单词生成过程不需要任何监督信息，因此可以应用于任何领域。

在医学领域，大量以语义类型、语义组等形式存在的真实标签信息已经被人工提取并集成到像UMLS这样的知识库中。我们的半监督方法是在神经网络训练过程中集成这种类型的真标签和所需的词，为每个词产生一个更好的向量表示。图1说明了这个想法。

![1631171837(1)](G:\Bulando\Bulando.github.io\imagesP\1631171837(1).jpg)

<center>图1：半监督词嵌入模型的一个实例。</center>

<h4>训练算法</h4>

Word2Vec模型中使用的训练算法首先学习在给定词向量(随机初始化)的情况下如何更新网络边的权值;然后它学习如何更新词向量，假设边缘上的权值是给定的(随机初始化)。它重复这两个步骤，直到输入语料库中的所有单词都被处理完。

我们生成一个真实的标签向量，并将其作为额外的输出节点集添加到神经网络中。该向量有148个条目，对应于133个医学语义类型和15个语义组。如果一个术语与某些语义类型和语义组相关联，则对应的条目将被设置为1，否则为0。增加输出层的大小会减慢计算速度。例如，与原始的Word2Vec负采样策略(150个负样本)相比，半监督方法将慢50%左右。

本节导出字向量和边权值的更新规则。由于半监督方法是对原始Word2vec模型的扩展，所以推导过程也是类似的。新的更新规则适用于第2节中提到的所有4个Word2Vec设置。本节中使用的符号在图2中定义。

![1631188609(1)](imagesP\1631188609(1).jpg)

可以证明下列公式总是成立的：
$$
∂(δ(x))/∂x = δ(x) · (1 − δ(x)) 
∂(log δ(x))/∂x = 1 − δ(x)
∂(log(1 − δ(x)))/∂x = −δ(x)
$$
