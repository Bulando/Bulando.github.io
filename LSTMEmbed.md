<h2>LSTMEmbed:Learning Word and Sense Representations from a Large Semantically Annotated Corpus with Long Short-Term Memories论文翻译</h2>

<h2>LSTMEmbed:长短期记忆网络从一个大的语义标记语料中学习词语和意思的表征</h2>


<h3>摘要</h3>

词汇嵌入是目前大多数自然语言处理任务中词汇事实上的标准表征，但近年来人们的注意力转向了向量表征，该表征捕获了词汇的不同含义，即意思。在本文中，我们探讨了双向LSTM模型从语义标注语料库中学习词义表示的能力。我们展示了使用一个意识到词序的体系结构(如LSTM)，使我们能够创建更好的表示。我们在评估语义表示的各种标准基准上评估我们提出的模型，在SemEval2014单词到语义的相似性任务上达到了最先进的性能。我们在http://lcl.uniroma1上发布代码和生成的单词和语义嵌入。它/ LSTMEmbed。

<h3>1 引言</h3>

出于交流效率的考虑，自然语言本质上是模棱两可的(Piantadosi et al.， 2012)。对我们人类来说，歧义不是问题，因为我们用共同的知识来填补空白，相互理解。因此，一个适合理解自然语言和与人类并肩工作的计算模型应该能够在一定程度上处理歧义(Navigli, 2018)。创建此类计算机系统的一个必要步骤是构建单词及其含义的正式表示，要么以大型知识库(如语义网络)的形式，要么以几何空间中的向量的形式(Navigli和Martelli, 2019)。

事实上，表示学习(Bengio et al.， 2013)多年来一直是自然语言处理的一个主要研究领域，而潜在的基于向量的表示，即嵌入，似乎是应对歧义的一个很好的候选对象。嵌入编码词汇和语义项目在低维连续空间。这些向量表示以特定于关系的向量偏移的形式捕获单词和意思的有用语法和语义信息，比如自然语言中的规则，以及它们之间的关系。最近的一些方法，如word2vec (Mikolov et al.， 2013)和GloVe (Pennington et al.， 2014)，能够从大型无注释语料库中学习有效的单词嵌入。但是，尽管单词嵌入已经为许多NLP任务的改进铺平了道路(Goldberg, 2017)，但它们仍然合并每个单词的各种含义，并让其主导意义在结果表征中压倒所有其他意义。相反，当这些嵌入学习方法被应用到语义标注的数据时，它们能够产生单词意义的嵌入(Iacobacci等人，2015)。

针对词汇多义性问题的一系列工作提出了创建语义嵌入，即将词汇中每个单词的不同意思分离出来(Huang et al.， 2012;Chen et al.， 2014;Iacobacci et al.， 2015;Flekova和Gurevych, 2016;Pilehvar和Collier, 2016;Mancini等人，2017年)。然而，这些方法的缺点之一是，它们在学习过程中没有考虑单词顺序。另一方面，基于rnn的考虑序列信息的基于单词的方法已经被提出，但是它们在嵌入的速度和质量上并不具有竞争力(Mikolov et al.， 2010;Mikolov and Zweig, 2012;Mesnil et al.， 2013)。

例如，在图1中，我们展示了文献中词汇和语义嵌入的t-SNE (Maaten和Hinton, 2008)投影的摘录：

![1632189694(1)](PaperImges\9-21LSTMEmbed\1632189694(1).jpg)

<font size=2>图1：一个例子的联合空间，字向量(正方形)和意义向量(点和叉)出现分离。图2：一个共享的单词空间(正方形)分布在空间和两个感觉簇(点和叉)。</font>

可以看出，首先，歧义词库的位置靠近与它同时出现的词(图中的方形)，其次，银行最接近的意思(点表示金融机构的意思，叉表示地理上的意思)聚集在两个分离的区域，与与它们相关的词(可能有歧义)没有明显的关联。一个更准确的表示方法是将单词向量分布在所有空间中，并定义与目标单词的每个含义相关的每组向量的簇(图2)。

最近，著名的长短期记忆(LSTM)神经网络模型已经成为学习序列表示的成功模型，从而为许多基于序列输入的自然语言处理任务(如句子和短语)提供了理想的解决方案(Hill et al.， 2016;Melamud et al.， 2016;Peters等人，2018年)。然而，迄今为止，lstm还没有被应用于与明确清单相关联的意义嵌入的有效创建。

在本文中，我们探讨了使用语义标记语料库的lstm体系结构在单词和语义的语义表示方面的能力。我们提出四个主要贡献：

- 我们引入了LSTMEmbed，这是一种基于双向LSTM的RNN模型，用于学习相同语义空间中的单词和语义嵌入，与最流行的考虑单词排序的任务方法不同。
- 我们提出了一个创新的想法，利用预先训练的嵌入，将其作为训练中的目标。
- 我们表明，基于lstm的模型不仅适合学习语境信息，而且也适合学习单个单词和意义的表示。
- 通过将我们的表示与知识资源连接起来，我们利用了已有的语义信息。

<h3>2 词和语义的嵌入</h3>

机器可解释的词的意义表示是许多自然语言处理任务的关键，因此获得良好的词的意义表示是该领域的一个重要研究目标，这一点从最近这方面的研究热潮中可以看出。

<h4>2.1 词嵌入</h4>

近年来，单词嵌入的流行度呈指数级增长。学习嵌入的模型通常基于神经网络，将单个单词表示为低维向量。Mikolov等人(2013,word2vec)表明，在原始文本上训练的神经网络学习到的单词表示可以对高度潜在的关系进行几何编码。典型的例子是王、男人+女人的矢量与皇后的诱导矢量非常接近。GloVe (Pennington et al.， 2014)是一种基于聚合全球词-词共现训练的替代方法，得到了类似的结果。虽然这些嵌入在单义词中是令人惊讶的好，但它们不能恰当地代表单词的非主导意义。例如，bar 1687和pub的表示应该是相似的，bar和stick的表示也应该是相似的，但是对于pub和stick有相似的表示是不可取的。针对这一问题提出了几种解决方法:Yu和Dredze(2014)提出了一种替代方法，除了使用共同特征外，还使用在语义资源中具有一定关联的单词，如PPDB (Ganitkevitch et al.， 2013)或WordNet (Miller, 1995)。Faruqui等人(2015)提出了一种适用于预处理嵌入的技术，在该技术中，向量被更新(翻新)，以使它们更类似于共享单词类型的向量，而不像共享单词类型的向量那样相似。从PPDB、WordNet和FrameNet等多种语义资源中提取词语类型(Baker et al.， 1998)。Melamud等人(2016)引入了context2vec，这是一种基于双向LSTM的句子和单词嵌入学习模型。该模型利用大型原始文本语料库训练神经模型，将整个句子上下文和目标词嵌入到同一个低维空间中。最后，Press和Wolf(2017)提出了一个基于word2vec的模型，其中嵌入是从输出最顶端的权值矩阵中提取的，而不是从输入的权值矩阵中，表明这些表示也是有效的词嵌入。

<h4>2.2 语义嵌入</h4>

与上述方法不同的是，每一种方法都旨在学习词汇项目的表示，意义嵌入将单个单词的意思表示为独立的向量。学习语义嵌入的主要方法之一是所谓的基于知识的方法，它依赖于预定义的语义清单，如WordNet、BabelNet1 (Navigli and Ponzetto, 2012)或Freebase2。SensEmbed3 (Iacobacci et al.， 2015)使用Babelfy4，一种最先进的词义消歧和实体链接工具，来构建一个语义标注语料库，该语料库又被用来用word2vec训练词义向量空间模型。SensEmbed利用了BabelNet的语义库的结构化知识以及从文本语料库中收集的分布式信息。由于该方法基于word2vec，模型在学习嵌入时缺乏单词排序。学习意义嵌入的另一种方法是从一组预先训练好的词嵌入开始，并将向量分解成各自的意义。这个想法是由Rothe和Schutze(2015)在AutoExtend中实现的，AutoExtend是一个从WordNet在共享空间中学习词素、词义和同义词的嵌入的系统。给定词是其词的和和，而同义词是其词的和的约束，同义词/词素嵌入与词嵌入在同一个向量空间中。AutoExtend基于一个自动编码器，一个模拟输入和输出向量的神经网络。然而，Mancini et al.(2017)指出，通过限制感官的表征，我们无法了解词汇与感官之间的关系。他们引入了SW2V模型，该模型将word2vec扩展为在相同的向量空间中学习单词和词义的嵌入，而不是通过对这两个表示的约束来学习。该模型是利用从WordNet和BabelNet中获取的大量语料库和知识建立的。他们的基本想法是，扩展word2vec的CBOW架构，以表示不同输入的单词和含义，并训练模型，以预测单词及其中间的含义。然而，基于word2vec的SW2V也缺乏语序的概念。

文献中的其他方法避免使用预定义的感觉量表。通过这种方法学习到的向量被识别为多原型嵌入，而不是感觉，因为这些向量只被识别为彼此不同，而对它们的内在感觉没有明确的识别。有几种方法使用了这一思想:Huang等人(2012)引入了一个模型，通过聚类词上下文表示来学习每个词的多个向量。Neelakantan等人(2014)扩展了word2vec，并加入了一个模块，如果一个单词出现的上下文与之前看到的相同单词的上下文太不相同，则该模块可以诱导新的意义向量。Li和Jurafsky(2015)也提出了类似的方法，他们使用中国餐厅过程作为一种诱导新感觉的方法。最后，Peters等人(2018)提出了基于深度双向语言模型的词在上下文中的表示模型ELMo。与其他相关方法相比，ELMo没有令牌字典，而是每个令牌由三个向量表示，其中两个是上下文相关的。一般来说，这些模型很难评估，因为它们缺乏与词汇语义资源的联系。

与此形成鲜明对比的是，我们在本文中提出的神经结构LSTMEmbed，旨在学习单词感觉的个体表征，链接到多语言词汇语义资源(如BabelNet)，同时处理单词排序，并使用预先训练的嵌入作为目标。

<h3>3 LSTMEmbed</h3>

许多学习嵌入的方法都是基于前馈神经网络的(第2节)。然而，最近，LSTMs作为一种新的、事实上的标准模型，通过上下文和语序感知，在NLP社区中获得了广泛的支持。在本节中，我们介绍了一种基于LSTM体系结构的联合学习单词和语义的新方法LSTMEmbed。

<h4>3.1 模型介绍</h4>

LSTMEmbed的核心是双向长短期记忆(BiLSTM)，这是一种递归神经网络(RNN)，它使用一组门来处理长期依赖。双向LSTM (BiLSTM)是原始LSTM的变体(Hochreiter和Schmidhuber, 1997)，特别适合于需要访问完整上下文的临时问题。在我们的例子中,我们使用一个架构类似于川和戴尔(2015),Kageb ack和Salomonsson(2016)和Melamud et al。(2016),在每一个时间步的状态BiLSTM由美国两LSTMs,集中在一个特定的步伐,接受输入在一个LSTM从以前的步伐,以及另一个LSTM中的未来时间步骤。当输出对应于分析的时间步长而不是整个上下文时，这尤其适用。

![1632190467(1)](PaperImges\9-21LSTMEmbed\1632190467(1).jpg)

<center>图3:LSTMEmbed体系结构</center>

图3与文献中其他基于lstm的方法相比，我们使用sensetagged文本来提供si W类型的输入上下文，…， si 1(前面的上下文)和si+1，…， si+W(后语境)，其中sj (j [i W，…， i+W])可以是一个单词，也可以是来自现有库存的意义标签(参见4.1节)。每个标记用对应的嵌入向量v(sj)∈R表示N，由一个共享的查找表给出，它使表示能够考虑到双方的上下文实际信息来学习。接下来，BiLSTM从左到右读取前一个上下文，从右到左读取后一个上下文:

![1632193287(1)](PaperImges\9-21LSTMEmbed\1632193287(1).jpg)

模型多了一层。两个lstm输出的级联通过稠密层线性投影：

![1632193330(1)](PaperImges\9-21LSTMEmbed\1632193330(1).jpg)

其中，Wo R 2m m为稠密层的权矩阵，m为LSTM的维数。

然后,模型比较outLSTMEmbed emb (si)、emb (si)是pretrained嵌入向量的目标令牌(参见4.1节的插图pretrained嵌入我们的实验中使用),而且,根据注释和pretrained的嵌入的使用,这可以是一个词,或者一个意义。在训练时，对网络的权值进行修改，使outLSTMEmbed与emb(si)之间的相似度最大化。损失函数1689是用余弦相似度计算的。

![1632193407(1)](PaperImges\9-21LSTMEmbed\1632193407(1).jpg)

训练结束后，我们从查表中得到单词和意思在同一个向量空间联合的潜在语义表示，即输入和LSTM之间的嵌入矩阵，其中一个条目s的嵌入向量由v(s)给出。

与标准BiLSTM相比，LSTMEmbed的新颖之处可以总结如下：

- 使用语义标注语料库进行语义学习。
- 学习单词和词义的表示，从单一的查找表中提取，在左右lstm之间共享。
- 一种新的学习方法，以一组预先训练好的嵌入为目标，使我们能够学习大量词汇的嵌入。

<h3>4 验证</h3>



