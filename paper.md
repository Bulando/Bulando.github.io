## 同义词预测的分层多任务词嵌入学习

### 摘要

同义词自动识别对于以实体为中心的文本挖掘和解释具有重要意义。由于现实生活中语言使用的高变异性，手工构建语义资源以覆盖所有同义词的成本过高，也可能导致覆盖范围有限。尽管有公共知识库，但除了英语外，它们只覆盖了有限的语言。本文以医学领域为研究对象，提出了一种自动加速中文医学同义词资源开发的方法，包括医疗专业人员的形式化实体和最终用户的嘈杂描述。基于分布式词表示的成功，我们设计了一个具有层次任务关系的多任务模型，以学习更多具有代表性的实体/术语嵌入，并将其应用于同义词预测。在该模型中，我们扩展了经典的跳跃图词嵌入模型，引入了一个辅助任务邻词语义类型预测，并根据任务复杂度对邻词语义类型进行分层组织。同时，我们将已有的医学术语同义词知识纳入到我们的词嵌入学习框架中。我们证明，与基线相比，从我们提出的多任务模型训练的嵌入在实体语义相关性评价、邻近词语义类型预测和同义词预测方面有显著的改进。此外，我们还建立了一个包含实体注释、描述和同义词对的大型中文医学文本语料库，为今后在这一方向上的研究提供了参考。

<h3>引言</h3>

同义词预测已经成为以实体为中心的文本挖掘和解释的一项重要任务[28,32]。借助同义词预测，可以将实体的非正式提及规范化为标准形式，这将大大减少终端用户和下游应用程序之间的通信差距。这些例子包括但不限于问答匹配[9]，信息检索[39]，医学诊断[20]。

从资源的角度来看，同义词预测的主要困难是语言使用的变异性高[5]，而知识库(KB)的覆盖率低[13]，尤其是英语以外的语言。例如在中医领域,概念食欲不振(翻译:食欲不振)已经超过20种同义词，但大多数人混肴使用,因为他们所使用的主要是患者没有多少医学知识。尽管可以利用最先进的命名实体识别工具[21]来发现更多的实体，但构建带有注释的标签的数据来进行非正式描述和用于训练同义词这方面只做了很少量的工作。

从建模的角度来看，同义词预测的关键问题是如何学习更有效的实体和描述的表示。有了高质量的语义表示，任何现成的分类器都可以用来预测同义词关系。最近，单词和实体嵌入方法[16,17,23,24]从大型语料库中学习单词的分布式向量表示，已经在数据挖掘社区中流行起来。对于英语，已经提出了一些基于词或字符嵌入的同义词预测方法[11,15,32]。

例如，Wang等人的[32]集成了术语的语义类型知识到词嵌入学习中，并将学习到的嵌入与其他句法特征结合起来用于同义词预测。尽管模型利用了语义类型知识，但它忽略了实体之间丰富的关系信息。Hasan等人[11]使用字符嵌入作为术语特征，将同义词预测任务视为神经网络的机器翻译问题，在给定源项的情况下，通过双向RNN生成目标同义词。这种复杂模型的一个限制是，它需要UMLS[18]中大量的标记数据(同义词对)，而中文中没有这样的公共资源。

我们假设，结合语义知识将学习更多有代表性的词嵌入，从而导致更准确的同义词预测。在本文中语义知识既包括实体的语义类型信息，也包括实体之间的语义关联信息。受Søgaard和Goldberg[29]和Hashimoto等人[12]的启发，他们显示出在连续层预测两个日益复杂但相关的任务的能力，我们提出了一个分层的多任务词嵌入模型，如图1所示。在底层，我们引入了一个辅助任务，根据目标词预测邻近词的语义类型。在上层，我们扩展了skip-gram模型[23]，以合并实体之间已有的同义词知识和下层任务的结果。这种层次化的结构使得我们不仅可以利用实体、语义类型和语义关系，而且可以在训练阶段相互增强这两个任务。

虽然我们的研究方法是通用的，但中文医学领域的语言使用变异性非常高，语义知识丰富，但知识库覆盖率低。我们的模型也可以应用于任何其他领域，在这些领域，外部知识非常多，语言使用的可变性非常高。实验结果表明，该模型在实体语义关联度评价、相邻词语义类型预测和同义词预测等方面具有较高的准确率。

综上所述，本文的贡献如下:

我们提出了一个层次的多任务词嵌入模型，充分利用医学领域的知识。通过引入邻词语义类型预测的辅助任务，为目标词嵌入提供更多信息。针对该模型，我们设计了另一种优化算法，取得了比现有算法更好的性能。

我们从专业医学教科书、维基百科和论坛中收集了大量的中文医学语料库(约10M句子)，目的是识别更多非正式医学描述和同义词对。从语料库中，我们识别和注释了151K个医疗实体和描述，涵盖18个类别，185K个高质量同义词对。带注释的数据集将帮助其他研究人员发现更多嘈杂和非正式的医学描述。据我们所知，该语料库是第一个既有实体标注又有同义词标注的汉语基准。

我们将我们的模型应用于400M对医学术语，并获得了约1M以前任何医疗资源中未见过的同义词候选词。新发现的同义词可以丰富已有的汉语知识库。此外，我们对我们的方法进行了深思熟虑的错误分析，为今后在这一方向的工作提供了思路。

<h3>相关工作</h3>

同义词抽取的重要性在生物医学和临床研究领域得到了公认[14,22]。早期的方法通常是非基于神经系统的方法。传统的技术包括使用词汇和句法特征[10]，基于双语对齐的方法[31]和术语图[25]上的随机漫步。为简单起见，我们不详细讨论它们。

在基于神经网络的方法中，单词嵌入技术被广泛用于同义词预测[11,15,32]。近年来，通过增强领域语义知识来增强词汇嵌入的研究越来越受到人们的关注。这种增强要么通过在训练阶段添加关系正则化来改变单词嵌入的目标[34,35]，要么对训练的单词向量进行后处理以适应语义关系[7]。对于这两种情况，只使用术语-术语关系，但忽略术语的语义类型信息。在表1中，我们总结了相关方法和我们的特点。

在所有基于嵌入的方法中，与我们最相似的是Wang et al.[32]和Yu and Dredze[37]。Wang等人在[32]中，在词汇嵌入训练过程中，将词汇的语义类型作为外标签信息加入。这种半监督的方法使得单词嵌入模型在生成所需单词时考虑所需的类型，这是在同一层次上有两个任务的多任务学习的一种特殊情况。在我们的模型中，我们不仅利用了术语的语义类型，而且还利用了术语-术语同义关系。Yu和Dredze[37]提出了一种关系约束的词嵌入模型，该模型通过最大化所有同义对的对数似然来利用术语-术语同义关系。虽然我们也使用术语之间的同义关系，但我们的工作和他们的工作有两个主要区别。第一个不同之处在于，我们的词嵌入模型是一个分层的多任务学习框架，它的辅助任务是预测词汇的语义类型。第二个区别是，我们采用了不同的正则化策略来强制同义对共享相似的嵌入，而不是最大化它们的对数似然。

另一个相关的研究方向是多任务学习(MTL)，它同时学习多个相关任务以提高泛化性能。MTL已被广泛应用于医疗信息学[8]、语音识别[30]和自然语言处理等领域[12,29]。特别地，我们的工作受到了Søgaard and Goldberg[29]和Hashimoto et al.[12]的启发，他们证明了通过考虑语言层次，在不同层次上定位不同任务的优势。例如，Hashimoto等人[12]构建了一个多任务模型，其中任务根据其复杂性递增(如词性标注实体分块依赖解析)。他们的工作和我们的工作的关键区别在于，我们的分层多任务模型不仅解决了两种预测任务，而且还利用了两种类型的语义知识。

<h3>3方法</h3>

在本节中，我们首先描述原始的skip-gram模型[23]，然后解释我们的分层多任务词嵌入模型。在详细介绍之前，我们在表2中概述了本文的表示法。

![](.\imagesP\1631015559(1).jpg)



#### 3.1 skip-gram嵌入模型

skip-gram模型[23]的目标是优化能有效预测给定目标词的邻近词的词嵌入。更正式地说，它使以下目标函数最小化

