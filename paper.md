## 同义词预测的分层多任务词嵌入学习

### 摘要

同义词自动识别对于以实体为中心的文本挖掘和解释具有重要意义。由于现实生活中语言使用的高变异性，手工构建语义资源以覆盖所有同义词的成本过高，也可能导致覆盖范围有限。尽管有公共知识库，但除了英语外，它们只覆盖了有限的语言。本文以医学领域为研究对象，提出了一种自动加速中文医学同义词资源开发的方法，包括医疗专业人员的形式化实体和最终用户的嘈杂描述。基于分布式词表示的成功，我们设计了一个具有层次任务关系的多任务模型，以学习更多具有代表性的实体/术语嵌入，并将其应用于同义词预测。在该模型中，我们扩展了经典的skip-gram词嵌入模型，引入了一个辅助任务邻词语义类型预测，并根据任务复杂度对邻词语义类型进行分层组织。同时，我们将已有的医学术语同义词知识纳入到我们的词嵌入学习框架中。我们证明，与基线相比，从我们提出的多任务模型训练的嵌入在实体语义相关性评价、邻近词语义类型预测和同义词预测方面有显著的改进。此外，我们还建立了一个包含实体注释、描述和同义词对的大型中文医学文本语料库，为今后在这一方向上的研究提供了参考。

<h3>引言</h3>

同义词预测已经成为以实体为中心的文本挖掘和解释的一项重要任务[28,32]。借助同义词预测，可以将实体的非正式提及规范化为标准形式，这将大大减少终端用户和下游应用程序之间的通信差距。这些例子包括但不限于问答匹配[9]，信息检索[39]，医学诊断[20]。

![hierarchical model](imagesP\hierarchical model.jpg)

图1:提出的分层多任务词嵌入模型综述。术语的语义型知识和术语同义知识在不同的层次上以不同的方式被利用。

从资源的角度来看，同义词预测的主要困难是语言使用的变异性高[5]，而知识库(KB)的覆盖率低[13]，尤其是英语以外的语言。例如在中医领域,概念食欲不振(翻译:食欲不振)已经超过20种同义词，但大多数人混肴使用,因为他们所使用的主要是患者没有多少医学知识。尽管可以利用最先进的命名实体识别工具[21]来发现更多的实体，但构建带有注释的标签的数据来进行非正式描述和用于训练同义词这方面只做了很少量的工作。

从建模的角度来看，同义词预测的关键问题是如何学习更有效的实体和描述的表示。有了高质量的语义表示，任何现成的分类器都可以用来预测同义词关系。最近，单词和实体嵌入方法[16,17,23,24]从大型语料库中学习单词的分布式向量表示，已经在数据挖掘社区中流行起来。对于英语，已经提出了一些基于词或字符嵌入的同义词预测方法[11,15,32]。

例如，Wang等人的[32]集成了术语的语义类型知识到词嵌入学习中，并将学习到的嵌入与其他句法特征结合起来用于同义词预测。尽管模型利用了语义类型知识，但它忽略了实体之间丰富的关系信息。Hasan等人[11]使用字符嵌入作为术语特征，将同义词预测任务视为神经网络的机器翻译问题，在给定源项的情况下，通过双向RNN生成目标同义词。这种复杂模型的一个限制是，它需要UMLS[18]中大量的标记数据(同义词对)，而中文中没有这样的公共资源。

我们假设，结合语义知识将学习更多有代表性的词嵌入，从而导致更准确的同义词预测。在本文中语义知识既包括实体的语义类型信息，也包括实体之间的语义关联信息。受Søgaard和Goldberg[29]和Hashimoto等人[12]的启发，他们显示出在连续层预测两个日益复杂但相关的任务的能力，我们提出了一个分层的多任务词嵌入模型，如图1所示。在底层，我们引入了一个辅助任务，根据目标词预测邻近词的语义类型。在上层，我们扩展了skip-gram模型[23]，以合并实体之间已有的同义词知识和下层任务的结果。这种层次化的结构使得我们不仅可以利用实体、语义类型和语义关系，而且可以在训练阶段相互增强这两个任务。

虽然我们的研究方法是通用的，但中文医学领域的语言使用变异性非常高，语义知识丰富，但知识库覆盖率低。我们的模型也可以应用于任何其他领域，在这些领域，外部知识非常多，语言使用的可变性非常高。实验结果表明，该模型在实体语义关联度评价、相邻词语义类型预测和同义词预测等方面具有较高的准确率。

综上所述，本文的贡献如下:

我们提出了一个层次的多任务词嵌入模型，充分利用医学领域的知识。通过引入邻词语义类型预测的辅助任务，为目标词嵌入提供更多信息。针对该模型，我们设计了一种优化算法，取得了比现有算法更好的性能。

我们从专业医学教科书、维基百科和论坛中收集了大量的中文医学语料库(约10M句子)，目的是识别更多非正式医学描述和同义词对。从语料库中，我们识别和注释了151K个医疗实体和描述，涵盖18个类别，185K个高质量同义词对。带注释的数据集将帮助其他研究人员发现更多嘈杂和非正式的医学描述。据我们所知，该语料库是第一个既有实体标注又有同义词标注的汉语基准。

我们将我们的模型应用于400M对医学术语，并获得了约1M以前任何医疗资源中未见过的同义词候选词。新发现的同义词可以丰富已有的汉语知识库。此外，我们对我们的方法进行了深思熟虑的错误分析，为今后在这一方向的工作提供了思路。

<h3>相关工作</h3>

同义词抽取的重要性在生物医学和临床研究领域得到了公认[14,22]。早期的方法通常是非基于神经系统的方法。传统的技术包括使用词汇和句法特征[10]，基于双语对齐的方法[31]和术语图[25]上的随机漫步。为简单起见，我们不详细讨论它们。

在基于神经网络的方法中，单词嵌入技术被广泛用于同义词预测[11,15,32]。近年来，通过增强领域语义知识来增强词汇嵌入的研究越来越受到人们的关注。这种增强要么通过在训练阶段添加关系正则化来改变单词嵌入的目标[34,35]，要么对训练的单词向量进行后处理以适应语义关系[7]。对于这两种情况，只使用术语-术语关系，但忽略术语的语义类型信息。在表1中，我们总结了相关方法和我们的特点。

![1631239224(1)](imagesP\1631239224(1).jpg)

表1 每个方法的特征。ST为语义类型，SR为同义关系，PP为后处理，MT为多任务。“x”表示方法具有一定的属性。

在所有基于嵌入的方法中，与我们最相似的是Wang et al.[32]和Yu and Dredze[37]。Wang等人在[32]中，在词汇嵌入训练过程中，将词汇的语义类型作为外标签信息加入。这种半监督的方法使得单词嵌入模型在生成所需单词时考虑所需的类型，这是在同一层次上有两个任务的多任务学习的一种特殊情况。在我们的模型中，我们不仅利用了术语的语义类型，而且还利用了术语-术语同义关系。Yu和Dredze[37]提出了一种关系约束的词嵌入模型，该模型通过最大化所有同义对的对数似然来利用术语-术语同义关系。虽然我们也使用术语之间的同义关系，但我们的工作和他们的工作有两个主要区别。第一个不同之处在于，我们的词嵌入模型是一个分层的多任务学习框架，它的辅助任务是预测词汇的语义类型。第二个区别是，我们采用了不同的正则化策略来强制同义对共享相似的嵌入，而不是最大化它们的对数似然。

另一个相关的研究方向是<font color=red>多任务学习(MTL)</font>，它同时学习多个相关任务以提高泛化性能。MTL已被广泛应用于医疗信息学[8]、语音识别[30]和自然语言处理等领域[12,29]。特别地，我们的工作受到了Søgaard and Goldberg[29]和Hashimoto et al.[12]的启发，他们证明了通过考虑语言层次，在不同层次上定位不同任务的优势。例如，Hashimoto等人[12]构建了一个多任务模型，其中任务根据其复杂性递增(如词性标注实体分块依赖解析)。他们的工作和我们的工作的关键区别在于，我们的分层多任务模型不仅解决了两种预测任务，而且还利用了两种类型的语义知识。

<h3>3方法</h3>

在本节中，我们首先描述原始的skip-gram模型[23]，然后解释我们的分层多任务词嵌入模型。在详细介绍之前，我们在表2中概述了本文的表示法。

![](imagesP\1631015559(1).jpg)



#### 3.1 skip-gram嵌入模型

skip-gram模型[23]的目标是优化能有效预测给定目标词的邻近词的词嵌入。更正式地说，它使以下目标函数最小化

![skip-gram](imagesP\skip-gram.jpg)

其中xt是目标单词，c是上下文窗口大小。利用softmax函数计算概率p(xO |xi)

![skip-gram1](imagesP\skip-gram1.jpg)

skip-gram模型交替更新V和W，输出隐藏的表示V作为最终的单词嵌入，其中Vi的第i行为单词xi s的嵌入向量。

<h4>3.2 分层多任务词嵌入</h4>

我们扩展了skip-gram模型[23]，引入了邻近词<font color=red>语义类型</font>预测的辅助任务。关键是了解相邻词的语义类型有助于相邻词的预测。例如，在医学领域，症状术语经常被其他症状术语或疾病术语所包围。在本文中，我们假设每个输入句子都被分割成一个单词/短语序列，并对医疗实体进行标注。预处理的优点是，我们可以直接训练嵌入医疗实体和描述，就像其他普通单词一样。

很明显，有三种方法来组织这两项任务。

- 将两个任务并行组织并共享共同的隐藏嵌入层，相当于神经网络中共享隐藏层的普通多任务学习
- 将两个任务分层组织，其中邻词预测任务放置在较低位置，邻词语义类型预测任务放置在较高位置
- 本文提出的层次结构如图1所示。它使邻词预测能够利用邻词语义类型预测和共享词嵌入的结果

我们选择最后一种结构有两个原因。首先，预测邻近单词比预测它们的语义类型更复杂。所有可能的相邻单词集的基数等于词汇表大小，比语义类型的基数大得多。其次，从语言学的角度来看，了解可能的语义类型有助于邻近词预测任务对属于这些类型的词进行集中预测。

在图2中，我们展示了我们建议的模型框架。在训练过程中，首先将目标词及其邻近词输入到输入层进行嵌入查找。同时，利用外部医学知识库(KB)对邻近词进行查询，确定其对应的语义类型。任务T1的训练数据是目标词及其邻近词类型的嵌入。注意，只有相邻的具有有效语义类型的单词(例如红色的单词)才会被输入T1。<font color=red>任务T2的输入是T1的语义类型概率分布和目标词与邻近词嵌入的概率分布的拼接。此外，将目标词的同义词作为外部知识输入T2。</font>

3.2.1  T1:相邻词语义类型预测。

给定输入的单词和它的嵌入向量，这个任务是在一个上下文窗口中预测它的相邻单词可能的语义类型。例如，在图2中，输入术语runny_nose被两个症状术语和一个疾病术语包围，上下文窗口大小为7。这个模型被期望为症状和疾病的语义类型分配更高的概率。

我们将任务T1转换为一个多标签分类问题，其中标签的数量等于语义类型的数量。虽然有很多复杂的多标签分类算法[38,40]，但我们使用的是二进制相关[27]，即为每个标签独立训练一个二进制分类器。选择二值关联的原因是，当损失函数为宏观平均测度[19]时，二值关联不仅计算有效，而且可以归纳出最优模型。特别地，我们最小化下面的正则化加权交叉熵目标。

![1631087918](imagesP\1631087918.jpg)



其中，当输入单词xt在训练集中有一个类型为cj的相邻单词时，yt j = 1，否则yt j = 0。Wj是cj类的正样本权重，可以作为相反的正负样本比率。条件概率 p(yt j |xt)定义为p(yt j |xt) = σ(UTj Vxt)。V0为上一纪元训练任务T2后的词嵌入，λ为正则化参数。为简单起见，我们省略了公式(3)中的偏差项，尽管我们在实现中包含了偏差项。



![model2](imagesP\model2.jpg)

图2 分层多任务词嵌入模型体系结构。注意，这两个任务都可以访问嵌入层。任务T1将使用语义类型信息，而任务T2将使用现有的同义词关系知识。右侧显示了如何将数据输入模型的示例。在这里Runny_nose是目标单词，上下文窗口大小为7。

(3)中的公式||V-V0||上2下F称为逐次正则化术语[12]，它惩罚了当前嵌入参数与从其他任务中学习到的参数的偏差。这样的正则化术语有助于防止参数在切换任务时变化过大，从而稳定训练过程。

注意，我们假设每个医学术语只有一个语义类型，这在医学领域是有效的，因为一个医疗实体很少有两个或更多的语义类型。例如，“阿司匹林”是一个药物实体，它不能包含疾病的语义类型。当将任务T1扩展到一个术语可能具有多个多重语义类型的其他领域时，可以使用上下文感知模型，如上下文依赖网络[26]。因为我们的重点是在医学领域，我们不讨论一般案例。

3.2.2  T2:邻近词预测

我们的邻词预测任务方法类似于最近利用先验知识(如释义、同义词)改进词嵌入的工作[7,34,37]。这些方法使用正则化术语修改原始的词嵌入目标，鼓励语义相关的词共享相似的词嵌入。不同之处在于，我们是在多任务环境下解决问题的，而他们的方法都是单任务学习。

特别地，我们用语义类型预测任务T1的结果来增加任务T2的输入，并利用逐次正则化项来促进两个任务参数之间的一定程度的一致性。

让<a href="https://www.codecogs.com/eqnedit.php?latex=\theta&space;=[V,U]" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\theta&space;=[V,U]" title="\theta =[V,U]" /></a>表示与任务T1相关的模型参数。任务T2被最小化的目标如下:

![1631259519(1)](G:\Bulando\Bulando.github.io\imagesP\1631259519(1).jpg)

其中S(xt)为来自外界知识的xt的同义词/释义集，fT 1(xt)为xt的邻域语义类型预测结果，λ1为同义词先验的正则化参数，θ0为当前训练epoch训练T1后的任务T1的参数。第二项是加强目标词xt与其已知同义词之间的词嵌入相似性，第三项是连续的正则化项，以稳定训练过程。

设φI = [VxI, fT 1(xI)]。定义在xI和fT 1(xI)条件下观察单词xO的概率为

![1631346795(1)](imagesP\1631346795(1).jpg)

公式(5)中的一个问题是计算归一化因子的复杂性很高，因为它涉及到对词汇表中所有单词的求和。为了解决这个问题，我们使用了负抽样(NEG)[24]将原始的一对所有多类目标转换为二分类目标。对于负抽样，Eq.(5)的负对数可以改写为: 

![1631347060(1)](imagesP\1631347060(1).jpg)

式中Pneq(xj)为xj的负样本集。将Eq.(6)代入Eq.(4)，我们得到任务T2的简化目标:

![1631346971(1)](imagesP\1631346971(1).jpg)

<h4>3.3 训练</h4>

He模型在一个外部知识库支持的大型文本语料库上训练，该语料库提供语义类型和术语-术语同义关系。我们使用小批量随机梯度下降(SGD)与一个计划，以衰减一半的学习率后，特定的全局步骤。

在每个epoch期间，优化从较低的任务迭代到较高的任务，如图2所示。具体来说，我们首先最小化Eq.(3)中的LT 1，在整个训练集上更新V和U，然后将优化后的V和U传递给上。通过在整个训练集上最小化Eq.(7)中的LT 2，我们更新ew,V和u，并在下一纪元开始时将V传递给较低级别任务。我们重复上述过程，直到达到预定义的纪元数，并输出V作为最终的单词嵌入。

之所以选择V而不是W作为最终的嵌入，是因为V是两个任务共享的，两个任务都会更新，而W只在训练相邻的单词预测任务时更新。因此，V携带更多关于实体类型的语义信息。我们也尝试使用w作为最终的嵌入，但它没有显示任何改进。

<h4>3.4 应用到同义词预测</h4>

在词汇嵌入学习中，虽然使用了同义词关系，但同义词的覆盖面有限。为了提取更多的同义对，可以训练更复杂的模型，或者使用简单的模型(例如线性支持向量机[6])，但包含更多信息特征。在本文中，我们采用后一种方法，因为我们的重点是学习更多有代表性的嵌入。

为了获取更多对同义词抽取有用的信息，我们跟随Wang等人[32]为术语对构建特征，包括扩展嵌入和词汇匹配特征。此外，我们增加了两个特征，1)词向量对之间的余弦相似度，2)两个词在字符串级之间的Jaro Winkler相似度[33]，在实体名称匹配任务[2]中获得了最好的性能。

<h3>4 实验</h3>

我们从9种教材、医学维基a +医院[1]和医学问答论坛2中收集了中文医学语料库。总的来说，语料库包含大约1000万个句子。我们遵循UMLS实体类型分类法3，但将低级语义类型合并到其高级概念(例如，详细的药物成分到药物)，并重命名几个语义类型，使众包验证更容易。共有18个类型:症状、疾病、药物、食物、治疗、手术、预防、医疗器械、科室、病因、身体部位、外伤、生物化学、检查及医学指标、生理学、心理学、医学调节、微生物学。

<h4>4.1医疗实体和同义词收集</h4>
在医疗维基网站上，我们收集了70K个专业实体。为了确定非正式医学术语，我们使用众包收集了30K份非正式医学描述。在200K个句子上训练由[36]实现的著名命名实体识别模型CNN-BiLSTM-CRF[21]，其中初始的100K个医学术语在BIOES方案[3]下进行标注。由于有18种语义类型，我们总共有73个NER标记。我们获得了90.7%的F1得分。

利用训练后的NER模型，我们从包含10M句子的大型医学语料库中发现了58K个新的实体和短语。经过众包验证后，我们保留了51K，并将它们与最初的100K合并，构建了一个包含属于18种语义类型的151K实体的医学词典。在图3中，我们提供了医学词典的汇总统计信息。

![1631271868(1)](imagesP\1631271868(1).jpg)

<center>图3 医学词典汇总</center>

<font color=red>为了收集用于词嵌入和同义词模型训练的初始同义词对，我们首先利用维基文本上的规则(如A a.k.a. B)和正则表达式来识别遵循一定模式的同义词。由于规则的覆盖范围有限，我们也使用无监督方法来收集更多的同义词。特别地，我们利用word2vec模型[24]训练文本语料上151K个实体的嵌入，然后利用基于密度的空间聚类(dbscan)[4]找到紧凑的聚类。</font>使用dbscan的原因是它不需要指定集群的数量，可以找到任何形状的集群。我们将两个样本在同一邻域内的距离阈值设定为较小的阈值ε = 2，将一个样本作为核心点的最小样本数设定为minPoint = 3。较小的距离阈值将有助于减少误报，达到更高的精度。

在获得同义词集群(30K)后，我们使用众包来保证每个集群只包含高质量的同义词。我们将所有注释者分成几个组，让两组人对同一批数据进行标注。对于分歧，第三组做出选择。平均注释器协议为0.80 0.09。我们总共得到185K个同义对。

<h4>4.2 实验数据处理</h4>

为了准备词嵌入的训练数据，我们使用了我们的医学词典定制的知名中文分词工具jieba4，将医学文本语料库中的句子切割成单词和实体/短语序列。这样的程序将确保词嵌入算法将医学术语作为一个整体来处理，并学习它们的表示。通过过滤出现次数少于5次的生僻词，并去除标点符号，我们得到了411,256个唯一的单词和短语。我们将分割后的语料库分为3部分:80%的训练、10%的验证和10%的邻近语义类型预测实验。

在所有同义对中，我们首先在第4.4小节中抽取包含3586个唯一实体的25k对样本进行实体语义相关性评估。其余160k对进一步分割为80%、10%、10%，用于4.6小节中同义词预测实验的训练、验证和测试。同义词对的80%分割也被用作我们的词汇嵌入训练的术语-术语知识。在表3中，我们总结了数据集的特征。

![1631272067(1)](imagesP\1631272067(1).jpg)

<center>表3 数据集的特征。-不分裂。语义相关度评价对数据从全部185K同义对中取样，不用于单词嵌入训练。</center>

<h4>4.3 实验参数设置</h4>

我们将单词向量长度d设为200，初始学习率设为0.001，相邻窗口大小设为5，小批量大小设为400,epoch数设为20，负样本数设为20。

找到最好的hyper-parameters我们的模型,我们运行一个参数搜索的组合连续正则化参数λ={0.1,0.5,1、2、8}和同义词前正规化λ1 ={0.01,0.05,0.1,0.5,1},和计算的平均双向余弦相似性的一对dev数据。我们发现这些参数并没有显著地改变性能(最多1.0%)。我们设定λ = 0.5和λ1 = 0.05，得到最佳结果。

为了公平比较，我们在80%的语料库数据(8M句)和术语-术语同义关系数据上对每种方法(我们的和竞争的方法)进行训练。此外，每种方法在单词向量长度、小批量大小、负样本数量和纪元数量方面都具有相同的设置。

我们将我们的方法与几种最先进的词嵌入方法进行了比较。

