## 同义词预测的分层多任务词嵌入学习

### 摘要

同义词自动识别对于以实体为中心的文本挖掘和解释具有重要意义。由于现实生活中语言使用的高变异性，手工构建语义资源以覆盖所有同义词的成本过高，也可能导致覆盖范围有限。尽管有公共知识库，但除了英语外，它们只覆盖了有限的语言。本文以医学领域为研究对象，提出了一种自动加速中文医学同义词资源开发的方法，包括医疗专业人员的形式化实体和最终用户的嘈杂描述。基于分布式词表示的成功，我们设计了一个具有层次任务关系的多任务模型，以学习更多具有代表性的实体/术语嵌入，并将其应用于同义词预测。在该模型中，我们扩展了经典的跳跃图词嵌入模型，引入了一个辅助任务邻词语义类型预测，并根据任务复杂度对邻词语义类型进行分层组织。同时，我们将已有的医学术语同义词知识纳入到我们的词嵌入学习框架中。我们证明，与基线相比，从我们提出的多任务模型训练的嵌入在实体语义相关性评价、邻近词语义类型预测和同义词预测方面有显著的改进。此外，我们还建立了一个包含实体注释、描述和同义词对的大型中文医学文本语料库，为今后在这一方向上的研究提供了参考。

<h3>引言</h3>

同义词预测已经成为以实体为中心的文本挖掘和解释的一项重要任务[28,32]。借助同义词预测，可以将实体的非正式提及规范化为标准形式，这将大大减少终端用户和下游应用程序之间的通信差距。这些例子包括但不限于问答匹配[9]，信息检索[39]，医学诊断[20]。

![hierarchical model](G:\Bulando\Bulando.github.io\imagesP\hierarchical model.jpg)

图1:提出的分层多任务词嵌入模型综述。术语的语义型知识和术语同义知识在不同的层次上以不同的方式被利用。

从资源的角度来看，同义词预测的主要困难是语言使用的变异性高[5]，而知识库(KB)的覆盖率低[13]，尤其是英语以外的语言。例如在中医领域,概念食欲不振(翻译:食欲不振)已经超过20种同义词，但大多数人混肴使用,因为他们所使用的主要是患者没有多少医学知识。尽管可以利用最先进的命名实体识别工具[21]来发现更多的实体，但构建带有注释的标签的数据来进行非正式描述和用于训练同义词这方面只做了很少量的工作。

从建模的角度来看，同义词预测的关键问题是如何学习更有效的实体和描述的表示。有了高质量的语义表示，任何现成的分类器都可以用来预测同义词关系。最近，单词和实体嵌入方法[16,17,23,24]从大型语料库中学习单词的分布式向量表示，已经在数据挖掘社区中流行起来。对于英语，已经提出了一些基于词或字符嵌入的同义词预测方法[11,15,32]。

例如，Wang等人的[32]集成了术语的语义类型知识到词嵌入学习中，并将学习到的嵌入与其他句法特征结合起来用于同义词预测。尽管模型利用了语义类型知识，但它忽略了实体之间丰富的关系信息。Hasan等人[11]使用字符嵌入作为术语特征，将同义词预测任务视为神经网络的机器翻译问题，在给定源项的情况下，通过双向RNN生成目标同义词。这种复杂模型的一个限制是，它需要UMLS[18]中大量的标记数据(同义词对)，而中文中没有这样的公共资源。

我们假设，结合语义知识将学习更多有代表性的词嵌入，从而导致更准确的同义词预测。在本文中语义知识既包括实体的语义类型信息，也包括实体之间的语义关联信息。受Søgaard和Goldberg[29]和Hashimoto等人[12]的启发，他们显示出在连续层预测两个日益复杂但相关的任务的能力，我们提出了一个分层的多任务词嵌入模型，如图1所示。在底层，我们引入了一个辅助任务，根据目标词预测邻近词的语义类型。在上层，我们扩展了skip-gram模型[23]，以合并实体之间已有的同义词知识和下层任务的结果。这种层次化的结构使得我们不仅可以利用实体、语义类型和语义关系，而且可以在训练阶段相互增强这两个任务。

虽然我们的研究方法是通用的，但中文医学领域的语言使用变异性非常高，语义知识丰富，但知识库覆盖率低。我们的模型也可以应用于任何其他领域，在这些领域，外部知识非常多，语言使用的可变性非常高。实验结果表明，该模型在实体语义关联度评价、相邻词语义类型预测和同义词预测等方面具有较高的准确率。

综上所述，本文的贡献如下:

我们提出了一个层次的多任务词嵌入模型，充分利用医学领域的知识。通过引入邻词语义类型预测的辅助任务，为目标词嵌入提供更多信息。针对该模型，我们设计了另一种优化算法，取得了比现有算法更好的性能。

我们从专业医学教科书、维基百科和论坛中收集了大量的中文医学语料库(约10M句子)，目的是识别更多非正式医学描述和同义词对。从语料库中，我们识别和注释了151K个医疗实体和描述，涵盖18个类别，185K个高质量同义词对。带注释的数据集将帮助其他研究人员发现更多嘈杂和非正式的医学描述。据我们所知，该语料库是第一个既有实体标注又有同义词标注的汉语基准。

我们将我们的模型应用于400M对医学术语，并获得了约1M以前任何医疗资源中未见过的同义词候选词。新发现的同义词可以丰富已有的汉语知识库。此外，我们对我们的方法进行了深思熟虑的错误分析，为今后在这一方向的工作提供了思路。

<h3>相关工作</h3>

同义词抽取的重要性在生物医学和临床研究领域得到了公认[14,22]。早期的方法通常是非基于神经系统的方法。传统的技术包括使用词汇和句法特征[10]，基于双语对齐的方法[31]和术语图[25]上的随机漫步。为简单起见，我们不详细讨论它们。

在基于神经网络的方法中，单词嵌入技术被广泛用于同义词预测[11,15,32]。近年来，通过增强领域语义知识来增强词汇嵌入的研究越来越受到人们的关注。这种增强要么通过在训练阶段添加关系正则化来改变单词嵌入的目标[34,35]，要么对训练的单词向量进行后处理以适应语义关系[7]。对于这两种情况，只使用术语-术语关系，但忽略术语的语义类型信息。在表1中，我们总结了相关方法和我们的特点。

在所有基于嵌入的方法中，与我们最相似的是Wang et al.[32]和Yu and Dredze[37]。Wang等人在[32]中，在词汇嵌入训练过程中，将词汇的语义类型作为外标签信息加入。这种半监督的方法使得单词嵌入模型在生成所需单词时考虑所需的类型，这是在同一层次上有两个任务的多任务学习的一种特殊情况。在我们的模型中，我们不仅利用了术语的语义类型，而且还利用了术语-术语同义关系。Yu和Dredze[37]提出了一种关系约束的词嵌入模型，该模型通过最大化所有同义对的对数似然来利用术语-术语同义关系。虽然我们也使用术语之间的同义关系，但我们的工作和他们的工作有两个主要区别。第一个不同之处在于，我们的词嵌入模型是一个分层的多任务学习框架，它的辅助任务是预测词汇的语义类型。第二个区别是，我们采用了不同的正则化策略来强制同义对共享相似的嵌入，而不是最大化它们的对数似然。

另一个相关的研究方向是多任务学习(MTL)，它同时学习多个相关任务以提高泛化性能。MTL已被广泛应用于医疗信息学[8]、语音识别[30]和自然语言处理等领域[12,29]。特别地，我们的工作受到了Søgaard and Goldberg[29]和Hashimoto et al.[12]的启发，他们证明了通过考虑语言层次，在不同层次上定位不同任务的优势。例如，Hashimoto等人[12]构建了一个多任务模型，其中任务根据其复杂性递增(如词性标注实体分块依赖解析)。他们的工作和我们的工作的关键区别在于，我们的分层多任务模型不仅解决了两种预测任务，而且还利用了两种类型的语义知识。

<h3>3方法</h3>

在本节中，我们首先描述原始的skip-gram模型[23]，然后解释我们的分层多任务词嵌入模型。在详细介绍之前，我们在表2中概述了本文的表示法。

![](imagesP\1631015559(1).jpg)



#### 3.1 skip-gram嵌入模型

skip-gram模型[23]的目标是优化能有效预测给定目标词的邻近词的词嵌入。更正式地说，它使以下目标函数最小化

![skip-gram](G:\Bulando\Bulando.github.io\imagesP\skip-gram.jpg)

其中xt是目标单词，c是上下文窗口大小。利用softmax函数计算概率p(xO |xi)

![skip-gram1](G:\Bulando\Bulando.github.io\imagesP\skip-gram1.jpg)

skip-gram模型交替更新V和W，输出隐藏的表示V作为最终的单词嵌入，其中Vi的第i行为单词xi s的嵌入向量。

<h4>3.2 分层多任务词嵌入</h4>

我们扩展了skip-gram模型[23]，引入了邻近词<font color=red>**语义类型**</font>预测的辅助任务。关键是了解相邻词的语义类型有助于相邻词的预测。例如，在医学领域，症状术语经常被其他症状术语或疾病术语所包围。在本文中，我们假设每个输入句子都被分割成一个单词/短语序列，并对医疗实体进行标注。预处理的优点是，我们可以直接训练嵌入医疗实体和描述，就像其他普通单词一样。

很明显，有三种方法来组织这两项任务。

- 将两个任务并行组织并共享共同的隐藏嵌入层，相当于神经网络中共享隐藏层的普通多任务学习
- 将两个任务分层组织，其中邻词预测任务放置在较低位置，邻词语义类型预测任务放置在较高位置
- 本文提出的层次结构如图1所示。它使邻词预测能够利用邻词语义类型预测和共享词嵌入的结果

我们选择最后一种结构有两个原因。首先，预测邻近单词比预测它们的语义类型更复杂。所有可能的相邻单词集的基数等于词汇表大小，比语义类型的基数大得多。其次，从语言学的角度来看，了解可能的语义类型有助于邻近词预测任务对属于这些类型的词进行集中预测。

在图2中，我们展示了我们建议的模型框架。在训练过程中，首先将目标词及其邻近词输入到输入层进行嵌入查找。同时，利用外部医学知识库对邻近词进行查询，确定其对应的语义类型。任务T1的训练数据是目标词及其邻近词类型的嵌入。注意，只有相邻的具有有效语义类型的单词(例如红色的单词)才会被输入T1。任务T2的输入是T1的语义类型概率分布和目标词与邻近词嵌入的概率分布的拼接。此外，将目标词的同义词作为外部知识输入T2。

3.2.1  T1:相邻词语义类型预测。给定输入的单词和它的嵌入向量，这个任务是在一个上下文窗口中预测它的相邻单词可能的语义类型。例如，在图2中，输入术语runny_nose被两个症状术语和一个疾病术语包围，上下文窗口大小为7。这个模型被期望为症状和疾病的语义类型分配更高的概率。

我们将任务T1转换为一个多标签分类问题，其中标签的数量等于语义类型的数量。虽然有很多复杂的多标签分类算法[38,40]，但我们使用的是二进制相关[27]，即为每个标签独立训练一个二进制分类器。选择二值关联的原因是，当损失函数为宏观平均测度[19]时，二值关联不仅计算有效，而且可以归纳出最优模型。特别地，我们最小化下面的正则化加权交叉熵目标。

![1631087918](G:\Bulando\Bulando.github.io\imagesP\1631087918.jpg)



其中，当输入单词xt在训练集中有一个类型为cj的相邻单词时，yt j = 1，否则yt j = 0。Wj是cj类的正样本权重，可以设置为正负样本比率的节。条件概率 p(yt j |xt)定义为p(yt j |xt) = σ(UTj Vxt)。V0为上一纪元训练任务T2后的词嵌入，λ为正则化参数。为简单起见，我们省略了Eq.(3)中的偏差项，尽管我们在实现中包含了偏差项



![model2](G:\Bulando\Bulando.github.io\imagesP\model2.jpg)

图2 分层多任务词嵌入模型体系结构。注意，这两个任务都可以访问嵌入层。任务T1将使用语义类型信息，而任务T2将使用现有的同义词关系知识。右侧显示了如何将数据输入模型的示例。在这里Runny_nose是目标单词，上下文窗口大小为7。

